name: Full End-to-End Pipeline Test

# Complete pipeline test:
# 1. Collect 100 balanced URLs (50 phishing + 50 legitimate)
# 2. Extract URL features locally, DNS/WHOIS features on EC2
# 3. Append to master dataset in S3
# 4. Train all 45 models (15 URL + 15 DNS + 15 WHOIS)
# 5. Find best ensemble combination
# 6. Test on a single unseen URL

on:
  workflow_dispatch:
    inputs:
      num_urls:
        description: 'Number of URLs to collect (will be balanced 50/50)'
        required: false
        default: '100'
      test_url:
        description: 'URL to test after training (leave empty for default)'
        required: false
        default: ''

env:
  AWS_REGION: us-east-1
  S3_BUCKET: phishnet-data
  EC2_INSTANCE_ID: i-0c8ab11c281702a22

jobs:
  # ========================================================================
  # STEP 1: Collect 100 Balanced URLs
  # ========================================================================
  collect-urls:
    name: "Step 1: Collect Balanced URLs"
    runs-on: ubuntu-latest
    outputs:
      batch_date: ${{ steps.batch.outputs.batch_date }}
      batch_file: ${{ steps.batch.outputs.batch_file }}
      phishing_count: ${{ steps.collect.outputs.phishing_count }}
      legit_count: ${{ steps.collect.outputs.legit_count }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download master dataset for duplicate checking
        run: |
          echo "üì• Downloading master dataset from S3 for duplicate checking..."
          mkdir -p data/processed
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv || {
            echo "‚ö†Ô∏è No master dataset found in S3, starting fresh"
            touch data/processed/phishing_features_complete.csv
          }

          if [ -s data/processed/phishing_features_complete.csv ]; then
            EXISTING_COUNT=$(tail -n +2 data/processed/phishing_features_complete.csv | wc -l | tr -d ' ')
            echo "üìä Master dataset contains $EXISTING_COUNT existing URLs"
          else
            echo "üìä Master dataset is empty - all URLs will be new"
          fi

      - name: Set batch info
        id: batch
        run: |
          BATCH_DATE=$(date +%Y%m%d_%H%M%S)
          BATCH_FILE="data/url_queue/batch_${BATCH_DATE}.csv"
          mkdir -p data/url_queue

          echo "batch_date=$BATCH_DATE" >> $GITHUB_OUTPUT
          echo "batch_file=$BATCH_FILE" >> $GITHUB_OUTPUT
          echo "üìÖ Batch: $BATCH_DATE"

      - name: Collect balanced URLs
        id: collect
        run: |
          NUM_URLS="${{ github.event.inputs.num_urls || '100' }}"
          echo "üîó Collecting $NUM_URLS balanced URLs..."

          python3 scripts/fetch_urls.py ${{ steps.batch.outputs.batch_file }} $NUM_URLS

          # Count URLs by type
          PHISHING=$(grep -c ",phishing" ${{ steps.batch.outputs.batch_file }} || echo "0")
          LEGIT=$(grep -c ",legitimate" ${{ steps.batch.outputs.batch_file }} || echo "0")
          TOTAL=$(tail -n +2 ${{ steps.batch.outputs.batch_file }} | wc -l | tr -d ' ')

          echo "phishing_count=$PHISHING" >> $GITHUB_OUTPUT
          echo "legit_count=$LEGIT" >> $GITHUB_OUTPUT

          echo "üìä Collection Results:"
          echo "   Phishing: $PHISHING"
          echo "   Legitimate: $LEGIT"
          echo "   Total: $TOTAL"

      - name: Upload batch as artifact
        uses: actions/upload-artifact@v4
        with:
          name: url-batch-${{ steps.batch.outputs.batch_date }}
          path: ${{ steps.batch.outputs.batch_file }}
          retention-days: 7

  # ========================================================================
  # STEP 2: Extract Features (URL locally, DNS/WHOIS on EC2)
  # ========================================================================
  extract-features:
    name: "Step 2: Extract All Features"
    runs-on: ubuntu-latest
    needs: collect-urls
    timeout-minutes: 120

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download batch artifact
        uses: actions/download-artifact@v4
        with:
          name: url-batch-${{ needs.collect-urls.outputs.batch_date }}
          path: data/url_queue/

      - name: Extract URL features (locally)
        run: |
          BATCH_FILE="data/url_queue/batch_${{ needs.collect-urls.outputs.batch_date }}.csv"
          URL_FEATURES="data/processed/url_features_${{ needs.collect-urls.outputs.batch_date }}.csv"

          echo "üîç Extracting URL features..."
          python3 scripts/extract_url_features.py $BATCH_FILE $URL_FEATURES

          ROWS=$(tail -n +2 $URL_FEATURES | wc -l | tr -d ' ')
          echo "‚úÖ Extracted URL features for $ROWS URLs"

      - name: Upload to S3 for EC2 processing
        run: |
          BATCH_DATE="${{ needs.collect-urls.outputs.batch_date }}"

          echo "üì§ Uploading to S3..."
          aws s3 cp data/url_queue/batch_${BATCH_DATE}.csv s3://${{ env.S3_BUCKET }}/queue/batch_${BATCH_DATE}.csv
          aws s3 cp data/processed/url_features_${BATCH_DATE}.csv s3://${{ env.S3_BUCKET }}/queue/url_features_${BATCH_DATE}.csv

          # Upload latest VM script
          aws s3 cp scripts/extract_vm_features_aws.py s3://${{ env.S3_BUCKET }}/scripts/

      - name: Start EC2 instance
        id: ec2
        run: |
          INSTANCE_STATE=$(aws ec2 describe-instances \
            --instance-ids ${{ env.EC2_INSTANCE_ID }} \
            --query 'Reservations[0].Instances[0].State.Name' \
            --output text)

          echo "EC2 state: $INSTANCE_STATE"

          if [ "$INSTANCE_STATE" = "stopped" ]; then
            echo "üöÄ Starting EC2 instance..."
            aws ec2 start-instances --instance-ids ${{ env.EC2_INSTANCE_ID }}
            aws ec2 wait instance-running --instance-ids ${{ env.EC2_INSTANCE_ID }}
            echo "‚è≥ Waiting for initialization..."
            sleep 60
          fi

          EC2_IP=$(aws ec2 describe-instances \
            --instance-ids ${{ env.EC2_INSTANCE_ID }} \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text)

          echo "ec2_ip=$EC2_IP" >> $GITHUB_OUTPUT
          echo "üñ•Ô∏è EC2 IP: $EC2_IP"

      - name: Extract DNS/WHOIS features on EC2
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ steps.ec2.outputs.ec2_ip }}
          username: ec2-user
          key: ${{ secrets.EC2_SSH_KEY }}
          command_timeout: 90m
          script: |
            cd /home/ec2-user/phishnet

            # Update script from S3
            aws s3 cp s3://${{ env.S3_BUCKET }}/scripts/extract_vm_features_aws.py scripts/

            # Run feature extraction (includes DNS + WHOIS + accumulation to master)
            python3.8 scripts/extract_vm_features_aws.py ${{ needs.collect-urls.outputs.batch_date }}

            echo "‚úÖ EC2 processing complete!"

      - name: Stop EC2 instance
        if: always()
        run: |
          echo "üõë Stopping EC2 to save costs..."
          aws ec2 stop-instances --instance-ids ${{ env.EC2_INSTANCE_ID }} || true

      - name: Download updated master from S3
        run: |
          mkdir -p data/processed
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

          ROWS=$(tail -n +2 data/processed/phishing_features_complete.csv | wc -l | tr -d ' ')
          echo "üìä Master dataset now has $ROWS total URLs"

      - name: Upload master as artifact
        uses: actions/upload-artifact@v4
        with:
          name: master-dataset-${{ needs.collect-urls.outputs.batch_date }}
          path: data/processed/phishing_features_complete.csv
          retention-days: 7

  # ========================================================================
  # STEP 3: Train All 45 Models
  # ========================================================================
  train-models:
    name: "Step 3: Train 45 Models"
    runs-on: ubuntu-latest
    needs: [collect-urls, extract-features]
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download master dataset from S3
        run: |
          mkdir -p data/processed
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

      - name: Prepare model-ready datasets
        run: |
          python3 << 'EOF'
          import pandas as pd
          import numpy as np

          df = pd.read_csv('data/processed/phishing_features_complete.csv')
          print(f"üìä Loaded {len(df)} rows")

          # Normalize labels
          if 'label' in df.columns:
              df['label'] = df['label'].astype(str).str.strip().str.lower()
              label_map = {
                  'phishing': 1, '1': 1, '1.0': 1,
                  'legitimate': 0, '0': 0, '0.0': 0
              }
              df['label'] = df['label'].map(label_map)
              df = df.dropna(subset=['label'])
              df['label'] = df['label'].astype(int)

          df.to_csv('data/processed/phishing_features_complete.csv', index=False)

          # Create imputed version
          df_imputed = df.copy()
          non_feature_cols = ['url', 'label', 'source', 'bucket']
          feature_cols = [col for col in df_imputed.columns if col not in non_feature_cols]

          for col in feature_cols:
              if df_imputed[col].dtype in ['float64', 'int64']:
                  df_imputed[col].fillna(df_imputed[col].mean(), inplace=True)
          df_imputed.fillna(0, inplace=True)
          df_imputed.to_csv('data/processed/phishing_features_complete_imputed.csv', index=False)

          print(f"‚úÖ Prepared {len(df)} rows for training")
          print(f"   Label distribution: {df['label'].value_counts().to_dict()}")
          EOF

          # Copy to modelready files
          cp data/processed/phishing_features_complete.csv data/processed/url_features_modelready.csv
          cp data/processed/phishing_features_complete.csv data/processed/dns_features_modelready.csv
          cp data/processed/phishing_features_complete.csv data/processed/whois_features_modelready.csv
          cp data/processed/phishing_features_complete_imputed.csv data/processed/url_features_modelready_imputed.csv
          cp data/processed/phishing_features_complete_imputed.csv data/processed/dns_features_modelready_imputed.csv
          cp data/processed/phishing_features_complete_imputed.csv data/processed/whois_features_modelready_imputed.csv

      - name: Train URL models (15 algorithms)
        run: |
          echo "üß† Training 15 URL models..."
          python3 scripts/train_url_model.py
          echo "‚úÖ URL models trained"

      - name: Train DNS models (15 algorithms)
        run: |
          echo "üß† Training 15 DNS models..."
          python3 scripts/train_dns_model.py
          echo "‚úÖ DNS models trained"

      - name: Train WHOIS models (15 algorithms)
        run: |
          echo "üß† Training 15 WHOIS models..."
          python3 scripts/train_whois_model.py
          echo "‚úÖ WHOIS models trained"

      - name: List trained models
        run: |
          echo "üìÅ Trained models:"
          ls -la models/*.pkl models/*.joblib 2>/dev/null | head -50 || echo "No .pkl/.joblib files"
          echo ""
          echo "üìä Model count:"
          echo "   URL models: $(ls models/url_*.pkl 2>/dev/null | wc -l)"
          echo "   DNS models: $(ls models/dns_*.pkl 2>/dev/null | wc -l)"
          echo "   WHOIS models: $(ls models/whois_*.pkl 2>/dev/null | wc -l)"

      - name: Upload trained models as artifact
        uses: actions/upload-artifact@v4
        with:
          name: trained-models-${{ needs.collect-urls.outputs.batch_date }}
          path: |
            models/*.pkl
            models/*.joblib
            models/*.json
          retention-days: 30

  # ========================================================================
  # STEP 4: Find Best Ensemble Combination
  # ========================================================================
  find-best-ensemble:
    name: "Step 4: Find Best Ensemble"
    runs-on: ubuntu-latest
    needs: [collect-urls, train-models]
    outputs:
      best_url_model: ${{ steps.ensemble.outputs.best_url_model }}
      best_dns_model: ${{ steps.ensemble.outputs.best_dns_model }}
      best_whois_model: ${{ steps.ensemble.outputs.best_whois_model }}
      ensemble_accuracy: ${{ steps.ensemble.outputs.ensemble_accuracy }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download trained models
        uses: actions/download-artifact@v4
        with:
          name: trained-models-${{ needs.collect-urls.outputs.batch_date }}
          path: models/

      - name: Download master dataset
        run: |
          mkdir -p data/processed
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

      - name: Prepare test data for ensemble evaluation
        run: |
          # Create test directory with model-ready data
          mkdir -p data/test

          # Copy the imputed model-ready files to test directory
          cp data/processed/url_features_modelready_imputed.csv data/test/ 2>/dev/null || echo "URL features not found"
          cp data/processed/dns_features_modelready_imputed.csv data/test/ 2>/dev/null || echo "DNS features not found"
          cp data/processed/whois_features_modelready_imputed.csv data/test/ 2>/dev/null || echo "WHOIS features not found"

          echo "üìÅ Test data prepared:"
          ls -la data/test/ 2>/dev/null || echo "No test files"

      - name: Find best ensemble combination
        id: ensemble
        run: |
          # Use the existing ensemble_comparison.py script
          echo "üîó Running ensemble comparison using existing script..."

          # Run the comparison script (it outputs to analysis/ensemble_comparison/)
          python3 scripts/ensemble_comparison.py --test-size 1000 --iterations 50 || {
            echo "‚ö†Ô∏è Ensemble comparison failed, creating fallback config..."
            mkdir -p models
            echo '{"best_url_model": "url_catboost.pkl", "best_dns_model": "dns_catboost.pkl", "best_whois_model": "whois_catboost.pkl", "note": "fallback config"}' > models/best_ensemble.json
          }

          # Copy latest results to models/best_ensemble.json
          LATEST_RESULTS=$(ls -t analysis/ensemble_comparison/comparison_*.json 2>/dev/null | head -1)
          if [ -n "$LATEST_RESULTS" ]; then
            cp "$LATEST_RESULTS" models/best_ensemble.json
            echo "‚úÖ Copied ensemble results from $LATEST_RESULTS"

            # Parse best ensemble for GitHub output
            python3 << EOF
          import json
          import os

          with open('models/best_ensemble.json') as f:
              results = json.load(f)

          # Get the best overall result (first in sorted list)
          if isinstance(results, list) and len(results) > 0:
              best = results[0]
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"best_url_model={best.get('models', {}).get('url', 'N/A')}\n")
                  f.write(f"best_dns_model={best.get('models', {}).get('dns', 'N/A')}\n")
                  f.write(f"best_whois_model={best.get('models', {}).get('whois', 'N/A')}\n")
                  f.write(f"ensemble_accuracy={best.get('accuracy', 0)}\n")
              print(f"üèÜ Best ensemble: {best.get('name', 'Unknown')}")
              print(f"   Accuracy: {best.get('accuracy', 0):.4f}")
              print(f"   F1 Score: {best.get('f1_score', 0):.4f}")
          else:
              print("‚ö†Ô∏è No valid ensemble results found")
          EOF
          else
            echo "‚ö†Ô∏è No ensemble results found"
          fi

      - name: Upload ensemble config
        uses: actions/upload-artifact@v4
        with:
          name: ensemble-config-${{ needs.collect-urls.outputs.batch_date }}
          path: models/best_ensemble.json
          retention-days: 30

  # ========================================================================
  # STEP 5: Deploy and Test on Unseen URL
  # ========================================================================
  deploy-and-test:
    name: "Step 5: Deploy & Test"
    runs-on: ubuntu-latest
    needs: [collect-urls, train-models, find-best-ensemble]
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download trained models
        uses: actions/download-artifact@v4
        with:
          name: trained-models-${{ needs.collect-urls.outputs.batch_date }}
          path: models/

      - name: Download ensemble config
        uses: actions/download-artifact@v4
        with:
          name: ensemble-config-${{ needs.collect-urls.outputs.batch_date }}
          path: models/

      - name: Test on unseen URL
        run: |
          TEST_URL="${{ github.event.inputs.test_url }}"

          # Default test URLs if none provided
          if [ -z "$TEST_URL" ]; then
            TEST_URL="https://www.microsoft.com"
          fi

          echo "üß™ Testing on unseen URL: $TEST_URL"

          python3 << EOF
          import sys
          sys.path.insert(0, '.')

          from src.api.predict_utils import predict_url_risk, predict_ensemble_risk

          test_url = "$TEST_URL"

          print(f"\n{'='*60}")
          print(f"TEST URL: {test_url}")
          print(f"{'='*60}")

          # URL model prediction
          url_prob, url_latency, url_model, _ = predict_url_risk(test_url)
          print(f"\nüìä URL Model:")
          print(f"   Model: {url_model}")
          print(f"   Risk Score: {url_prob:.4f}")
          print(f"   Verdict: {'PHISHING' if url_prob >= 0.5 else 'SAFE'}")
          print(f"   Latency: {url_latency:.0f}ms")

          # Ensemble prediction
          ens_prob, ens_latency, ens_model, debug = predict_ensemble_risk(test_url)
          print(f"\nüîó Ensemble Model:")
          print(f"   Risk Score: {ens_prob:.4f}")
          print(f"   Verdict: {'PHISHING' if ens_prob >= 0.5 else 'SAFE'}")
          print(f"   Latency: {ens_latency:.0f}ms")

          if debug:
              print(f"\n   Debug info:")
              for k, v in debug.items():
                  if isinstance(v, float):
                      print(f"      {k}: {v:.4f}")
                  else:
                      print(f"      {k}: {v}")

          print(f"\n{'='*60}")
          print("‚úÖ END-TO-END TEST COMPLETE!")
          print(f"{'='*60}")
          EOF

      - name: Print ensemble results
        run: |
          echo ""
          echo "üìä FINAL RESULTS SUMMARY"
          echo "========================"
          echo "Best URL Model: ${{ needs.find-best-ensemble.outputs.best_url_model }}"
          echo "Best DNS Model: ${{ needs.find-best-ensemble.outputs.best_dns_model }}"
          echo "Best WHOIS Model: ${{ needs.find-best-ensemble.outputs.best_whois_model }}"
          echo "Ensemble Accuracy: ${{ needs.find-best-ensemble.outputs.ensemble_accuracy }}"
          echo ""
          echo "URLs collected: ${{ needs.collect-urls.outputs.phishing_count }} phishing + ${{ needs.collect-urls.outputs.legit_count }} legitimate"

      - name: Commit trained models
        run: |
          git config user.name "EpbAiD"
          git config user.email "eeshanpbhanap@gmail.com"

          git add models/

          if git diff --staged --quiet; then
            echo "No model changes to commit"
            exit 0
          fi

          BATCH_DATE="${{ needs.collect-urls.outputs.batch_date }}"
          ACCURACY="${{ needs.find-best-ensemble.outputs.ensemble_accuracy }}"

          git commit -m "[e2e-test] Train and deploy models (batch $BATCH_DATE)

          - Collected ${{ needs.collect-urls.outputs.phishing_count }} phishing + ${{ needs.collect-urls.outputs.legit_count }} legitimate URLs
          - Trained 45 models (15 URL + 15 DNS + 15 WHOIS)
          - Best ensemble accuracy: ${ACCURACY}
          - Best URL model: ${{ needs.find-best-ensemble.outputs.best_url_model }}
          - Best DNS model: ${{ needs.find-best-ensemble.outputs.best_dns_model }}
          - Best WHOIS model: ${{ needs.find-best-ensemble.outputs.best_whois_model }}"

          git push
