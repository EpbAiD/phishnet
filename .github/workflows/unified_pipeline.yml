name: Unified ML Pipeline

# Single configurable pipeline - reads .github/pipeline_config.yml
# Change frequencies in config file instead of editing workflow
# Uses AWS (EC2, S3, RDS) for infrastructure

on:
  schedule:
    # Daily schedule (controlled by config)
    - cron: '0 14 * * *'     # 9 AM EST daily
    # Weekly schedule (controlled by config)
    - cron: '0 14 * * 0'     # Sunday 9 AM EST

  workflow_dispatch:
    inputs:
      override_stage:
        description: 'Force run specific stage (data/features/training/deploy/all)'
        required: false
        default: 'all'
      num_urls:
        description: 'Override num_urls from config'
        required: false

env:
  AWS_REGION: us-east-1
  S3_BUCKET: phishnet-data
  EC2_INSTANCE_ID: i-0c8ab11c281702a22
  EC2_HOST: 3.92.88.199
  CONFIG_FILE: .github/pipeline_config.yml

jobs:
  # ========================================================================
  # JOB 1: Load Configuration
  # ========================================================================
  load-config:
    name: Load Pipeline Configuration
    runs-on: ubuntu-latest
    outputs:
      data_collection_freq: ${{ steps.parse.outputs.data_collection_freq }}
      feature_extraction_freq: ${{ steps.parse.outputs.feature_extraction_freq }}
      model_training_freq: ${{ steps.parse.outputs.model_training_freq }}
      deployment_freq: ${{ steps.parse.outputs.deployment_freq }}
      num_urls: ${{ steps.parse.outputs.num_urls }}
      mode: ${{ steps.parse.outputs.mode }}
      should_run_data: ${{ steps.decide.outputs.should_run_data }}
      should_run_training: ${{ steps.decide.outputs.should_run_training }}
      should_run_deployment: ${{ steps.decide.outputs.should_run_deployment }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Parse configuration
        id: parse
        run: |
          # Read config file
          DATA_FREQ=$(grep -A 2 "data_collection:" $CONFIG_FILE | grep "frequency:" | awk '{print $2}' | tr -d '"')
          FEATURE_FREQ=$(grep -A 2 "feature_extraction:" $CONFIG_FILE | grep "frequency:" | awk '{print $2}' | tr -d '"')
          TRAINING_FREQ=$(grep -A 2 "model_training:" $CONFIG_FILE | grep "frequency:" | awk '{print $2}' | tr -d '"')
          DEPLOY_FREQ=$(grep -A 2 "deployment:" $CONFIG_FILE | grep "frequency:" | awk '{print $2}' | tr -d '"')
          NUM_URLS=$(grep "num_urls:" $CONFIG_FILE | head -1 | awk '{print $2}')
          MODE=$(grep "mode:" $CONFIG_FILE | awk '{print $2}' | tr -d '"')

          echo "data_collection_freq=$DATA_FREQ" >> $GITHUB_OUTPUT
          echo "feature_extraction_freq=$FEATURE_FREQ" >> $GITHUB_OUTPUT
          echo "model_training_freq=$TRAINING_FREQ" >> $GITHUB_OUTPUT
          echo "deployment_freq=$DEPLOY_FREQ" >> $GITHUB_OUTPUT
          echo "num_urls=${NUM_URLS}" >> $GITHUB_OUTPUT
          echo "mode=$MODE" >> $GITHUB_OUTPUT

          echo "üìã Configuration loaded:"
          echo "  Mode: $MODE"
          echo "  Data collection: $DATA_FREQ"
          echo "  Feature extraction: $FEATURE_FREQ"
          echo "  Model training: $TRAINING_FREQ"
          echo "  Deployment: $DEPLOY_FREQ"
          echo "  URLs per run: $NUM_URLS"

      - name: Decide what to run
        id: decide
        run: |
          # Determine if today is Sunday (for weekly runs)
          IS_SUNDAY=$(date +%u)  # 7 = Sunday

          # Override from manual trigger
          OVERRIDE="${{ github.event.inputs.override_stage }}"

          # Data collection decision
          DATA_FREQ="${{ steps.parse.outputs.data_collection_freq }}"
          if [ "$OVERRIDE" = "all" ] || [ "$OVERRIDE" = "data" ]; then
            SHOULD_RUN_DATA=true
          elif [ "$DATA_FREQ" = "daily" ]; then
            SHOULD_RUN_DATA=true
          elif [ "$DATA_FREQ" = "weekly" ] && [ "$IS_SUNDAY" = "7" ]; then
            SHOULD_RUN_DATA=true
          else
            SHOULD_RUN_DATA=false
          fi

          # Training decision
          TRAINING_FREQ="${{ steps.parse.outputs.model_training_freq }}"
          if [ "$OVERRIDE" = "all" ] || [ "$OVERRIDE" = "training" ]; then
            SHOULD_RUN_TRAINING=true
          elif [ "$TRAINING_FREQ" = "daily" ]; then
            SHOULD_RUN_TRAINING=true
          elif [ "$TRAINING_FREQ" = "weekly" ] && [ "$IS_SUNDAY" = "7" ]; then
            SHOULD_RUN_TRAINING=true
          else
            SHOULD_RUN_TRAINING=false
          fi

          # Deployment decision (usually same as training)
          DEPLOY_FREQ="${{ steps.parse.outputs.deployment_freq }}"
          if [ "$OVERRIDE" = "all" ] || [ "$OVERRIDE" = "deploy" ]; then
            SHOULD_RUN_DEPLOYMENT=true
          elif [ "$DEPLOY_FREQ" = "daily" ]; then
            SHOULD_RUN_DEPLOYMENT=true
          elif [ "$DEPLOY_FREQ" = "weekly" ] && [ "$IS_SUNDAY" = "7" ]; then
            SHOULD_RUN_DEPLOYMENT=true
          else
            SHOULD_RUN_DEPLOYMENT=false
          fi

          echo "should_run_data=$SHOULD_RUN_DATA" >> $GITHUB_OUTPUT
          echo "should_run_training=$SHOULD_RUN_TRAINING" >> $GITHUB_OUTPUT
          echo "should_run_deployment=$SHOULD_RUN_DEPLOYMENT" >> $GITHUB_OUTPUT

          echo "üéØ Run decisions:"
          echo "  Data collection: $SHOULD_RUN_DATA"
          echo "  Model training: $SHOULD_RUN_TRAINING"
          echo "  Deployment: $SHOULD_RUN_DEPLOYMENT"

  # ========================================================================
  # JOB 2: Data Collection & Feature Extraction (AWS)
  # ========================================================================
  data-and-features:
    name: Collect Data & Extract Features
    runs-on: ubuntu-latest
    needs: load-config
    if: needs.load-config.outputs.should_run_data == 'true'
    timeout-minutes: 180

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set batch info
        id: batch
        run: |
          BATCH_DATE=$(date +%Y%m%d)
          BATCH_NAME="batch_${BATCH_DATE}.csv"
          BATCH_FILE="data/url_queue/${BATCH_NAME}"

          echo "batch_date=$BATCH_DATE" >> $GITHUB_ENV
          echo "batch_name=$BATCH_NAME" >> $GITHUB_ENV
          echo "batch_file=$BATCH_FILE" >> $GITHUB_ENV

      - name: Collect URLs
        run: |
          NUM_URLS="${{ github.event.inputs.num_urls || needs.load-config.outputs.num_urls }}"
          echo "Collecting $NUM_URLS URLs..."
          python3 scripts/fetch_urls.py ${{ env.batch_file }} $NUM_URLS

      - name: Extract URL Features
        run: |
          echo "Extracting URL features..."
          python3 scripts/extract_url_features.py ${{ env.batch_file }} data/processed/url_features_${{ env.batch_date }}.csv

      - name: Upload batch to S3 for EC2 processing
        run: |
          echo "Uploading batch and URL features to S3..."
          aws s3 cp ${{ env.batch_file }} s3://${{ env.S3_BUCKET }}/queue/${{ env.batch_name }}
          aws s3 cp data/processed/url_features_${{ env.batch_date }}.csv s3://${{ env.S3_BUCKET }}/queue/url_features_${{ env.batch_date }}.csv

      - name: Start EC2 instance
        run: |
          INSTANCE_STATE=$(aws ec2 describe-instances \
            --instance-ids ${{ env.EC2_INSTANCE_ID }} \
            --query 'Reservations[0].Instances[0].State.Name' \
            --output text)

          echo "EC2 instance state: $INSTANCE_STATE"

          if [ "$INSTANCE_STATE" = "stopped" ]; then
            echo "Starting EC2 instance..."
            aws ec2 start-instances --instance-ids ${{ env.EC2_INSTANCE_ID }}
            aws ec2 wait instance-running --instance-ids ${{ env.EC2_INSTANCE_ID }}
            echo "Waiting for instance to initialize..."
            sleep 60
          else
            echo "EC2 instance already running"
          fi

          # Get current public IP (may change after restart)
          EC2_IP=$(aws ec2 describe-instances \
            --instance-ids ${{ env.EC2_INSTANCE_ID }} \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text)
          echo "EC2 Public IP: $EC2_IP"
          echo "ec2_ip=$EC2_IP" >> $GITHUB_ENV

      - name: Upload VM script to S3
        run: |
          echo "Uploading updated VM script..."
          aws s3 cp scripts/extract_vm_features_aws.py s3://${{ env.S3_BUCKET }}/scripts/

      - name: EC2 Processing (DNS/WHOIS extraction + accumulation)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.ec2_ip }}
          username: ec2-user
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            cd /home/ec2-user/phishnet
            aws s3 cp s3://${{ env.S3_BUCKET }}/scripts/extract_vm_features_aws.py scripts/
            python3.8 scripts/extract_vm_features_aws.py ${{ env.batch_date }}
            echo "‚úÖ EC2 processing complete - master dataset updated in S3"

      - name: Stop EC2 instance
        if: always()
        run: |
          echo "Stopping EC2 instance to save costs..."
          aws ec2 stop-instances --instance-ids ${{ env.EC2_INSTANCE_ID }} || true

      - name: Download master dataset from S3 for training
        run: |
          echo "Downloading updated master dataset from S3..."
          mkdir -p data/processed
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

      - name: Upload master dataset as artifact
        uses: actions/upload-artifact@v4
        with:
          name: features-master
          path: data/processed/phishing_features_complete.csv
          retention-days: 1

  # ========================================================================
  # JOB 3: Model Training
  # ========================================================================
  train-models:
    name: Train ML Models
    runs-on: ubuntu-latest
    needs: [load-config, data-and-features]
    if: needs.load-config.outputs.should_run_training == 'true'
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt boto3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download master dataset from S3
        run: |
          echo "Downloading master dataset from S3..."
          mkdir -p data/processed
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

      - name: Prepare model-ready datasets
        run: |
          # Encode labels and create imputed version
          python3 << 'EOF'
          import pandas as pd
          import numpy as np

          df = pd.read_csv('data/processed/phishing_features_complete.csv')
          print(f"üìä Loaded {len(df)} rows from S3")

          # Normalize labels - handle mixed string/numeric formats
          if 'label' in df.columns:
              # Convert to string first for consistent mapping
              df['label'] = df['label'].astype(str).str.strip().str.lower()

              # Map all variations to 0/1
              label_map = {
                  'phishing': 1, '1': 1, '1.0': 1,
                  'legitimate': 0, '0': 0, '0.0': 0
              }
              df['label'] = df['label'].map(label_map)

              # Drop any rows that couldn't be mapped
              before = len(df)
              df = df.dropna(subset=['label'])
              dropped = before - len(df)
              if dropped > 0:
                  print(f"‚ö†Ô∏è  Dropped {dropped} rows with invalid labels")

              df['label'] = df['label'].astype(int)

          df.to_csv('data/processed/phishing_features_complete.csv', index=False)

          # Create imputed version
          df_imputed = df.copy()
          non_feature_cols = ['url', 'label', 'source', 'bucket']
          feature_cols = [col for col in df_imputed.columns if col not in non_feature_cols]

          for col in feature_cols:
              if df_imputed[col].dtype in ['float64', 'int64']:
                  df_imputed[col].fillna(df_imputed[col].mean(), inplace=True)
          df_imputed.fillna(0, inplace=True)
          df_imputed.to_csv('data/processed/phishing_features_complete_imputed.csv', index=False)

          print(f"‚úÖ Prepared {len(df)} rows for training")
          print(f"   Label distribution: {df['label'].value_counts().to_dict()}")
          EOF

          # Copy to modelready files
          cp data/processed/phishing_features_complete.csv data/processed/url_features_modelready.csv
          cp data/processed/phishing_features_complete.csv data/processed/dns_features_modelready.csv
          cp data/processed/phishing_features_complete.csv data/processed/whois_features_modelready.csv
          cp data/processed/phishing_features_complete_imputed.csv data/processed/url_features_modelready_imputed.csv
          cp data/processed/phishing_features_complete_imputed.csv data/processed/dns_features_modelready_imputed.csv
          cp data/processed/phishing_features_complete_imputed.csv data/processed/whois_features_modelready_imputed.csv

      - name: Train URL model
        run: python3 scripts/train_url_model.py

      - name: Train DNS model
        run: python3 scripts/train_dns_model.py

      - name: Train WHOIS model
        run: python3 scripts/train_whois_model.py

      - name: Commit trained models
        run: |
          git config user.name "EpbAiD"
          git config user.email "eeshanpbhanap@gmail.com"

          git add models/

          if git diff --staged --quiet; then
            echo "No model changes to commit"
            exit 0
          fi

          MODE="${{ needs.load-config.outputs.mode }}"
          ROWS=$(wc -l < data/processed/phishing_features_complete.csv)

          git commit -m "[$MODE] Train models on $ROWS URLs

          - URL models: 15 algorithms
          - DNS models: 15 algorithms
          - WHOIS models: 15 algorithms
          - Total: 45 models updated"

          git push

  # ========================================================================
  # JOB 4: Update Validation Counter
  # ========================================================================
  update-validation:
    name: Update Validation Progress
    runs-on: ubuntu-latest
    needs: [load-config, train-models]
    if: needs.load-config.outputs.mode == 'validation'
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Increment validation counter
        run: |
          COMPLETED=$(grep "runs_completed:" .github/pipeline_config.yml | awk '{print $2}')
          NEW_COUNT=$((COMPLETED + 1))

          sed -i "s/runs_completed: $COMPLETED/runs_completed: $NEW_COUNT/" .github/pipeline_config.yml

          git config user.name "EpbAiD"
          git config user.email "eeshanpbhanap@gmail.com"
          git add .github/pipeline_config.yml
          git commit -m "Validation progress: Run $NEW_COUNT of 10 complete"

          # Pull first to avoid conflicts with model training job
          git pull --rebase origin main
          git push

          echo "‚úÖ Validation run $NEW_COUNT of 10 complete"

          if [ $NEW_COUNT -eq 10 ]; then
            echo "üéâ ALL 10 VALIDATION RUNS COMPLETE!"
            echo "üí° Change frequencies in .github/pipeline_config.yml to production mode"
          fi
