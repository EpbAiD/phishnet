name: 10-Run Validation (Progressive Learning)

# Run 10 iterations of the E2E pipeline:
# Each run collects 100 NEW unique URLs (checking against master dataset)
# Features are extracted and appended to master
# 45 models are trained on the growing dataset
# Best ensemble is evaluated
# Final test shows progressive improvement

on:
  workflow_dispatch:
    inputs:
      run_count:
        description: 'Number of validation runs (1-10)'
        required: false
        default: '10'
      urls_per_run:
        description: 'New URLs to collect per run'
        required: false
        default: '100'

env:
  AWS_REGION: us-east-1
  S3_BUCKET: phishnet-data
  EC2_INSTANCE_ID: i-0c8ab11c281702a22

jobs:
  # ========================================================================
  # Single job that runs all 10 iterations sequentially
  # ========================================================================
  validation:
    name: "Run 10 Validation Iterations"
    runs-on: ubuntu-latest
    timeout-minutes: 600  # 10 hours max

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt boto3 pyarrow

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup SSH key for EC2
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key
          # Verify key was written
          echo "SSH key file size: $(wc -c < ~/.ssh/ec2_key) bytes"
          head -1 ~/.ssh/ec2_key

      - name: Run validation loop
        run: |
          TOTAL_RUNS="${{ github.event.inputs.run_count || '10' }}"
          URLS_PER_RUN="${{ github.event.inputs.urls_per_run || '100' }}"

          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘         10-RUN PROGRESSIVE LEARNING VALIDATION               â•‘"
          echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
          echo "â•‘  Runs: $TOTAL_RUNS | URLs per run: $URLS_PER_RUN                          â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""

          # Create results tracking file
          mkdir -p analysis/validation_runs
          RESULTS_FILE="analysis/validation_runs/results_$(date +%Y%m%d_%H%M%S).csv"
          echo "run,total_urls,new_urls_added,phishing,legitimate,best_accuracy,best_f1,best_ensemble" > $RESULTS_FILE

          for RUN_NUM in $(seq 1 $TOTAL_RUNS); do
            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "                    RUN $RUN_NUM of $TOTAL_RUNS"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""

            BATCH_DATE=$(date +%Y%m%d_%H%M%S)
            BATCH_FILE="data/url_queue/batch_${BATCH_DATE}.csv"
            mkdir -p data/url_queue data/processed

            # ------------------------------------------------------------
            # STEP 1: Download master for duplicate checking
            # ------------------------------------------------------------
            echo "ğŸ“¥ Step 1: Downloading master dataset..."
            aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv 2>/dev/null || {
              echo "   Creating new master dataset"
              echo "url,label" > data/processed/phishing_features_complete.csv
            }

            BEFORE_COUNT=$(tail -n +2 data/processed/phishing_features_complete.csv 2>/dev/null | wc -l | tr -d ' ' || echo "0")
            echo "   Master contains $BEFORE_COUNT existing URLs"

            # ------------------------------------------------------------
            # STEP 2: Collect NEW unique URLs
            # ------------------------------------------------------------
            echo ""
            echo "ğŸ”— Step 2: Collecting $URLS_PER_RUN NEW unique URLs..."
            python3 scripts/fetch_urls.py $BATCH_FILE $URLS_PER_RUN || {
              echo "âš ï¸ URL collection failed, trying with smaller batch"
              python3 scripts/fetch_urls.py $BATCH_FILE 50 || true
            }

            if [ ! -f "$BATCH_FILE" ]; then
              echo "âŒ Failed to collect URLs, skipping this run"
              continue
            fi

            # Count results
            TOTAL=$(tail -n +2 $BATCH_FILE 2>/dev/null | wc -l | tr -d ' ' || echo "0")
            PHISHING=$(grep -c ",phishing" $BATCH_FILE 2>/dev/null || echo "0")
            LEGIT=$(grep -c ",legitimate" $BATCH_FILE 2>/dev/null || echo "0")

            echo "   Collected: $TOTAL URLs (Phishing: $PHISHING, Legitimate: $LEGIT)"

            if [ "$TOTAL" -lt 5 ]; then
              echo "âš ï¸ Too few URLs collected ($TOTAL), skipping this run"
              continue
            fi

            # ------------------------------------------------------------
            # STEP 3: Extract URL features locally
            # ------------------------------------------------------------
            echo ""
            echo "ğŸ” Step 3: Extracting URL features locally..."
            URL_FEATURES="data/processed/url_features_${BATCH_DATE}.csv"
            python3 scripts/extract_url_features.py $BATCH_FILE $URL_FEATURES

            # ------------------------------------------------------------
            # STEP 4: Upload to S3 and process on EC2
            # ------------------------------------------------------------
            echo ""
            echo "ğŸ“¤ Step 4: Uploading to S3..."
            aws s3 cp $BATCH_FILE s3://${{ env.S3_BUCKET }}/queue/batch_${BATCH_DATE}.csv
            aws s3 cp $URL_FEATURES s3://${{ env.S3_BUCKET }}/queue/url_features_${BATCH_DATE}.csv
            aws s3 cp scripts/extract_vm_features_aws.py s3://${{ env.S3_BUCKET }}/scripts/

            echo ""
            echo "ğŸ–¥ï¸ Step 5: Starting EC2 for DNS/WHOIS extraction..."

            # Check current state and handle accordingly
            INSTANCE_STATE=$(aws ec2 describe-instances --instance-ids ${{ env.EC2_INSTANCE_ID }} --query 'Reservations[0].Instances[0].State.Name' --output text)
            echo "   Current state: $INSTANCE_STATE"

            if [ "$INSTANCE_STATE" = "stopped" ]; then
              echo "   Starting instance..."
              aws ec2 start-instances --instance-ids ${{ env.EC2_INSTANCE_ID }}
              aws ec2 wait instance-running --instance-ids ${{ env.EC2_INSTANCE_ID }}
              sleep 60  # Wait for SSH to be ready
            elif [ "$INSTANCE_STATE" = "stopping" ]; then
              echo "   Waiting for instance to stop..."
              aws ec2 wait instance-stopped --instance-ids ${{ env.EC2_INSTANCE_ID }}
              echo "   Starting instance..."
              aws ec2 start-instances --instance-ids ${{ env.EC2_INSTANCE_ID }}
              aws ec2 wait instance-running --instance-ids ${{ env.EC2_INSTANCE_ID }}
              sleep 60  # Wait for SSH to be ready
            elif [ "$INSTANCE_STATE" = "pending" ]; then
              echo "   Waiting for instance to start..."
              aws ec2 wait instance-running --instance-ids ${{ env.EC2_INSTANCE_ID }}
              sleep 60  # Wait for SSH to be ready
            elif [ "$INSTANCE_STATE" = "running" ]; then
              echo "   Instance already running"
              sleep 10  # Brief wait for any ongoing operations
            else
              echo "   Unknown state: $INSTANCE_STATE, trying to start..."
              aws ec2 start-instances --instance-ids ${{ env.EC2_INSTANCE_ID }} || true
              aws ec2 wait instance-running --instance-ids ${{ env.EC2_INSTANCE_ID }}
              sleep 60
            fi

            # Get the public IP dynamically
            EC2_IP=$(aws ec2 describe-instances --instance-ids ${{ env.EC2_INSTANCE_ID }} --query 'Reservations[0].Instances[0].PublicIpAddress' --output text)
            echo "   EC2 Public IP: $EC2_IP"

            echo ""
            echo "ğŸ” Step 6: Extracting DNS/WHOIS features on EC2..."
            ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key ec2-user@$EC2_IP << EOFEC2
              cd /home/ec2-user/phishnet
              aws s3 cp s3://${{ env.S3_BUCKET }}/queue/batch_${BATCH_DATE}.csv batch.csv
              aws s3 cp s3://${{ env.S3_BUCKET }}/queue/url_features_${BATCH_DATE}.csv url_features.csv
              aws s3 cp s3://${{ env.S3_BUCKET }}/scripts/extract_vm_features_aws.py extract_vm_features_aws.py
              aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv master.csv || echo "url,label" > master.csv

              python3 extract_vm_features_aws.py batch.csv url_features.csv master.csv --mode batch

              aws s3 cp master.csv s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv
              echo "âœ… Updated master dataset"
          EOFEC2

            echo ""
            echo "ğŸ›‘ Stopping EC2 instance..."
            aws ec2 stop-instances --instance-ids ${{ env.EC2_INSTANCE_ID }}

            # ------------------------------------------------------------
            # STEP 7: Download updated master and train models
            # ------------------------------------------------------------
            echo ""
            echo "ğŸ“¥ Step 7: Downloading updated master..."
            aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

            AFTER_COUNT=$(tail -n +2 data/processed/phishing_features_complete.csv | wc -l | tr -d ' ')
            NEW_ADDED=$((AFTER_COUNT - BEFORE_COUNT))
            echo "   Master now has $AFTER_COUNT URLs (+$NEW_ADDED new)"

            echo ""
            echo "ğŸ‹ï¸ Step 8: Preparing model-ready datasets..."
            python3 << 'EOFPY'
          import pandas as pd
          import numpy as np

          df = pd.read_csv('data/processed/phishing_features_complete.csv')
          print(f"Loaded {len(df)} rows")

          # Encode labels
          if 'label' in df.columns:
              df['label_encoded'] = (df['label'] == 'phishing').astype(int)
          elif 'label_encoded' not in df.columns:
              df['label_encoded'] = 0

          # Define feature groups
          url_features = [c for c in df.columns if c.startswith(('url_', 'has_', 'is_', 'count_', 'len_', 'ratio_', 'domain_')) and not c.startswith(('dns_', 'whois_'))]
          dns_features = [c for c in df.columns if c.startswith('dns_')]
          whois_features = [c for c in df.columns if c.startswith('whois_')]

          print(f"Features: URL={len(url_features)}, DNS={len(dns_features)}, WHOIS={len(whois_features)}")

          for name, features in [('url', url_features), ('dns', dns_features), ('whois', whois_features)]:
              cols = features + ['label_encoded']
              df_subset = df[cols].copy()
              df_imputed = df_subset.fillna(0)
              df_imputed.to_csv(f'data/processed/{name}_features_modelready_imputed.csv', index=False)
          EOFPY

            echo ""
            echo "ğŸ‹ï¸ Step 9: Training 45 models..."
            mkdir -p models
            python3 << 'EOFPY'
          import pandas as pd
          import numpy as np
          from sklearn.model_selection import train_test_split
          from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier
          from sklearn.linear_model import LogisticRegression
          from sklearn.tree import DecisionTreeClassifier
          from sklearn.neighbors import KNeighborsClassifier
          from sklearn.naive_bayes import GaussianNB
          from sklearn.svm import SVC
          from sklearn.neural_network import MLPClassifier
          from catboost import CatBoostClassifier
          from lightgbm import LGBMClassifier
          from xgboost import XGBClassifier
          import joblib
          import json
          import warnings
          warnings.filterwarnings('ignore')

          models = {
              'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
              'gradient_boost': GradientBoostingClassifier(n_estimators=100, random_state=42),
              'adaboost': AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME'),
              'logistic': LogisticRegression(max_iter=1000, random_state=42),
              'decision_tree': DecisionTreeClassifier(random_state=42),
              'knn': KNeighborsClassifier(n_neighbors=5),
              'naive_bayes': GaussianNB(),
              'svm': SVC(probability=True, random_state=42),
              'mlp': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),
              'catboost': CatBoostClassifier(iterations=100, random_state=42, verbose=0),
              'lightgbm': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),
              'xgboost': XGBClassifier(n_estimators=100, random_state=42, verbosity=0),
              'extra_trees': ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1),
              'bagging': BaggingClassifier(n_estimators=50, random_state=42),
              'voting_base': RandomForestClassifier(n_estimators=50, random_state=42),
          }

          results = {'url': {}, 'dns': {}, 'whois': {}}

          for feature_type in ['url', 'dns', 'whois']:
              print(f"Training {feature_type.upper()} models...")
              df = pd.read_csv(f'data/processed/{feature_type}_features_modelready_imputed.csv')
              X = df.drop('label_encoded', axis=1)
              y = df['label_encoded']

              if len(y.unique()) < 2:
                  print(f"  Skipping {feature_type} - only one class")
                  continue

              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

              for name, model in models.items():
                  try:
                      model.fit(X_train, y_train)
                      accuracy = model.score(X_test, y_test)
                      results[feature_type][name] = accuracy
                      joblib.dump(model, f'models/{feature_type}_{name}.pkl')
                  except Exception as e:
                      print(f"  {name}: {e}")

          with open('models/training_results.json', 'w') as f:
              json.dump(results, f, indent=2)

          print("\nBest models:")
          for feature_type in ['url', 'dns', 'whois']:
              if results[feature_type]:
                  best = max(results[feature_type].items(), key=lambda x: x[1])
                  print(f"  {feature_type.upper()}: {best[0]} ({best[1]:.4f})")
          EOFPY

            echo ""
            echo "ğŸ”— Step 10: Evaluating ensemble..."
            python3 scripts/ensemble_comparison.py --test-size 500 --iterations 20 2>/dev/null || echo "Ensemble evaluation skipped"

            # Get best results
            BEST_ACC="N/A"
            BEST_F1="N/A"
            BEST_NAME="N/A"
            if [ -f "models/training_results.json" ]; then
              BEST_ACC=$(python3 -c "import json; r=json.load(open('models/training_results.json')); print(max(max(v.values()) for v in r.values() if v))" 2>/dev/null || echo "N/A")
            fi

            LATEST_ENSEMBLE=$(ls -t analysis/ensemble_comparison/comparison_*.json 2>/dev/null | head -1)
            if [ -n "$LATEST_ENSEMBLE" ]; then
              BEST_F1=$(python3 -c "import json; r=json.load(open('$LATEST_ENSEMBLE')); print(r[0].get('f1_score', 'N/A') if isinstance(r, list) else 'N/A')" 2>/dev/null || echo "N/A")
              BEST_NAME=$(python3 -c "import json; r=json.load(open('$LATEST_ENSEMBLE')); print(r[0].get('name', 'N/A') if isinstance(r, list) else 'N/A')" 2>/dev/null || echo "N/A")
            fi

            # Record results
            echo "$RUN_NUM,$AFTER_COUNT,$NEW_ADDED,$PHISHING,$LEGIT,$BEST_ACC,$BEST_F1,$BEST_NAME" >> $RESULTS_FILE

            echo ""
            echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
            echo "â•‘               RUN $RUN_NUM COMPLETE                              â•‘"
            echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
            echo "â•‘  Dataset: $BEFORE_COUNT â†’ $AFTER_COUNT URLs (+$NEW_ADDED new)              "
            echo "â•‘  Best Accuracy: $BEST_ACC                                    "
            echo "â•‘  Best Ensemble: $BEST_NAME                                   "
            echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

            # Small delay between runs
            sleep 5
          done

          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘            ALL VALIDATION RUNS COMPLETE                      â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "Results saved to: $RESULTS_FILE"
          cat $RESULTS_FILE

      - name: Upload validation results
        uses: actions/upload-artifact@v4
        with:
          name: validation-results-${{ github.run_id }}
          path: |
            analysis/validation_runs/
            models/training_results.json
          retention-days: 30
