name: 10-Run Validation (Progressive Learning)

# Run 10 iterations of the E2E pipeline:
# Each run collects 100 NEW unique URLs (checking against master dataset)
# Features are extracted and appended to master
# 45 models are trained on the growing dataset
# Best ensemble is evaluated
# Final test shows progressive improvement

on:
  workflow_dispatch:
    inputs:
      run_count:
        description: 'Number of validation runs (1-10)'
        required: false
        default: '10'
      urls_per_run:
        description: 'New URLs to collect per run'
        required: false
        default: '100'

env:
  AWS_REGION: us-east-1
  S3_BUCKET: phishnet-data
  EC2_INSTANCE_ID: i-0c8ab11c281702a22

jobs:
  # ========================================================================
  # Initialize validation tracking
  # ========================================================================
  init:
    name: "Initialize Validation"
    runs-on: ubuntu-latest
    outputs:
      total_runs: ${{ steps.init.outputs.total_runs }}
      start_time: ${{ steps.init.outputs.start_time }}

    steps:
      - name: Initialize
        id: init
        run: |
          TOTAL_RUNS="${{ github.event.inputs.run_count || '10' }}"
          START_TIME=$(date +%Y%m%d_%H%M%S)

          echo "total_runs=$TOTAL_RUNS" >> $GITHUB_OUTPUT
          echo "start_time=$START_TIME" >> $GITHUB_OUTPUT

          echo "ğŸš€ Starting $TOTAL_RUNS validation runs"
          echo "ğŸ“… Start time: $START_TIME"

  # ========================================================================
  # Run validation iterations
  # ========================================================================
  validation-run:
    name: "Run ${{ matrix.run_num }}/10"
    runs-on: ubuntu-latest
    needs: init
    timeout-minutes: 180
    strategy:
      max-parallel: 1  # Run sequentially so each builds on previous
      matrix:
        run_num: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    steps:
      - name: Check if should run
        id: check
        run: |
          TOTAL_RUNS="${{ needs.init.outputs.total_runs }}"
          CURRENT_RUN="${{ matrix.run_num }}"

          if [ "$CURRENT_RUN" -le "$TOTAL_RUNS" ]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "âœ… Running iteration $CURRENT_RUN of $TOTAL_RUNS"
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "â­ï¸ Skipping iteration $CURRENT_RUN (only $TOTAL_RUNS runs requested)"
          fi

      - name: Checkout code
        if: steps.check.outputs.should_run == 'true'
        uses: actions/checkout@v4

      - name: Set up Python
        if: steps.check.outputs.should_run == 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        if: steps.check.outputs.should_run == 'true'
        run: pip install -r requirements.txt boto3

      - name: Configure AWS credentials
        if: steps.check.outputs.should_run == 'true'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # --------------------------------------------------------------------
      # STEP 1: Download master & collect NEW URLs
      # --------------------------------------------------------------------
      - name: Download master dataset for duplicate checking
        if: steps.check.outputs.should_run == 'true'
        id: master
        run: |
          echo "ğŸ“¥ Downloading master dataset from S3..."
          mkdir -p data/processed

          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv || {
            echo "âš ï¸ No master dataset found, starting fresh"
            echo "url,label" > data/processed/phishing_features_complete.csv
          }

          EXISTING_COUNT=$(tail -n +2 data/processed/phishing_features_complete.csv | wc -l | tr -d ' ')
          echo "existing_urls=$EXISTING_COUNT" >> $GITHUB_OUTPUT
          echo "ğŸ“Š Master contains $EXISTING_COUNT existing URLs"

      - name: Collect NEW balanced URLs
        if: steps.check.outputs.should_run == 'true'
        id: collect
        run: |
          NUM_URLS="${{ github.event.inputs.urls_per_run || '100' }}"
          BATCH_DATE=$(date +%Y%m%d_%H%M%S)
          BATCH_FILE="data/url_queue/batch_${BATCH_DATE}.csv"

          mkdir -p data/url_queue
          echo "batch_date=$BATCH_DATE" >> $GITHUB_OUTPUT

          echo "ğŸ”— Collecting $NUM_URLS NEW unique URLs (Run ${{ matrix.run_num }})..."
          python3 scripts/fetch_urls.py $BATCH_FILE $NUM_URLS

          # Count results
          TOTAL=$(tail -n +2 $BATCH_FILE | wc -l | tr -d ' ')
          PHISHING=$(grep -c ",phishing" $BATCH_FILE || echo "0")
          LEGIT=$(grep -c ",legitimate" $BATCH_FILE || echo "0")

          echo "total_urls=$TOTAL" >> $GITHUB_OUTPUT
          echo "phishing_count=$PHISHING" >> $GITHUB_OUTPUT
          echo "legit_count=$LEGIT" >> $GITHUB_OUTPUT

          echo "ğŸ“Š Collected $TOTAL NEW URLs:"
          echo "   Phishing: $PHISHING"
          echo "   Legitimate: $LEGIT"

      # --------------------------------------------------------------------
      # STEP 2: Extract URL features locally
      # --------------------------------------------------------------------
      - name: Extract URL features
        if: steps.check.outputs.should_run == 'true'
        run: |
          BATCH_DATE="${{ steps.collect.outputs.batch_date }}"
          BATCH_FILE="data/url_queue/batch_${BATCH_DATE}.csv"
          URL_FEATURES="data/processed/url_features_${BATCH_DATE}.csv"

          echo "ğŸ” Extracting URL features locally..."
          python3 scripts/extract_url_features.py $BATCH_FILE $URL_FEATURES

          ROWS=$(tail -n +2 $URL_FEATURES | wc -l | tr -d ' ')
          echo "âœ… Extracted features for $ROWS URLs"

      # --------------------------------------------------------------------
      # STEP 3: Upload to S3 and extract DNS/WHOIS on EC2
      # --------------------------------------------------------------------
      - name: Upload to S3 for EC2 processing
        if: steps.check.outputs.should_run == 'true'
        run: |
          BATCH_DATE="${{ steps.collect.outputs.batch_date }}"

          echo "ğŸ“¤ Uploading to S3..."
          aws s3 cp data/url_queue/batch_${BATCH_DATE}.csv s3://${{ env.S3_BUCKET }}/queue/batch_${BATCH_DATE}.csv
          aws s3 cp data/processed/url_features_${BATCH_DATE}.csv s3://${{ env.S3_BUCKET }}/queue/url_features_${BATCH_DATE}.csv
          aws s3 cp scripts/extract_vm_features_aws.py s3://${{ env.S3_BUCKET }}/scripts/

      - name: Start EC2 instance
        if: steps.check.outputs.should_run == 'true'
        run: |
          echo "ğŸ–¥ï¸ Starting EC2 instance..."
          aws ec2 start-instances --instance-ids ${{ env.EC2_INSTANCE_ID }}
          aws ec2 wait instance-running --instance-ids ${{ env.EC2_INSTANCE_ID }}

          # Wait for SSH
          sleep 60
          echo "âœ… EC2 instance running"

      - name: Extract DNS/WHOIS features on EC2
        if: steps.check.outputs.should_run == 'true'
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ubuntu
          key: ${{ secrets.EC2_SSH_KEY }}
          port: 22
          command_timeout: 90m
          script: |
            echo "ğŸ”§ Setting up EC2 environment..."
            cd /home/ubuntu

            # Download files from S3
            BATCH_DATE="${{ steps.collect.outputs.batch_date }}"
            aws s3 cp s3://${{ env.S3_BUCKET }}/queue/batch_${BATCH_DATE}.csv batch.csv
            aws s3 cp s3://${{ env.S3_BUCKET }}/queue/url_features_${BATCH_DATE}.csv url_features.csv
            aws s3 cp s3://${{ env.S3_BUCKET }}/scripts/extract_vm_features_aws.py extract_vm_features_aws.py

            # Download existing master
            aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv master.csv || echo "url,label" > master.csv

            echo "ğŸ“Š Current master: $(wc -l < master.csv) rows"

            # Run feature extraction
            echo "ğŸ” Extracting DNS/WHOIS features..."
            python3 extract_vm_features_aws.py batch.csv url_features.csv master.csv --mode batch

            # Upload updated master
            echo "ğŸ“¤ Uploading updated master to S3..."
            aws s3 cp master.csv s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv

            NEW_COUNT=$(wc -l < master.csv)
            echo "âœ… Master now has $NEW_COUNT rows"

      - name: Stop EC2 instance
        if: steps.check.outputs.should_run == 'true'
        run: |
          aws ec2 stop-instances --instance-ids ${{ env.EC2_INSTANCE_ID }}
          echo "ğŸ›‘ EC2 instance stopped"

      # --------------------------------------------------------------------
      # STEP 4: Download updated master and train models
      # --------------------------------------------------------------------
      - name: Download updated master
        if: steps.check.outputs.should_run == 'true'
        id: updated_master
        run: |
          echo "ğŸ“¥ Downloading updated master dataset..."
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

          NEW_TOTAL=$(tail -n +2 data/processed/phishing_features_complete.csv | wc -l | tr -d ' ')
          echo "new_total=$NEW_TOTAL" >> $GITHUB_OUTPUT
          echo "ğŸ“Š Master now contains $NEW_TOTAL total URLs"

      - name: Prepare model-ready datasets
        if: steps.check.outputs.should_run == 'true'
        run: |
          python3 << 'EOF'
          import pandas as pd
          import numpy as np

          df = pd.read_csv('data/processed/phishing_features_complete.csv')
          print(f"Loaded {len(df)} rows")

          # Encode labels
          if 'label' in df.columns:
              df['label_encoded'] = (df['label'] == 'phishing').astype(int)
          elif 'label_encoded' not in df.columns:
              df['label_encoded'] = 0

          # Define feature groups
          url_features = [c for c in df.columns if c.startswith(('url_', 'has_', 'is_', 'count_', 'len_', 'ratio_', 'domain_')) and not c.startswith(('dns_', 'whois_'))]
          dns_features = [c for c in df.columns if c.startswith('dns_')]
          whois_features = [c for c in df.columns if c.startswith('whois_')]

          print(f"URL features: {len(url_features)}")
          print(f"DNS features: {len(dns_features)}")
          print(f"WHOIS features: {len(whois_features)}")

          # Create model-ready datasets
          for name, features in [('url', url_features), ('dns', dns_features), ('whois', whois_features)]:
              cols = features + ['label_encoded']
              df_subset = df[cols].copy()
              df_imputed = df_subset.fillna(0)
              df_imputed.to_csv(f'data/processed/{name}_features_modelready_imputed.csv', index=False)
              print(f"Saved {name} features: {len(df_imputed)} rows, {len(features)} features")

          print("âœ… Model-ready datasets created")
          EOF

      - name: Train 45 models
        if: steps.check.outputs.should_run == 'true'
        id: train
        run: |
          echo "ğŸ‹ï¸ Training 45 models (15 URL + 15 DNS + 15 WHOIS)..."
          mkdir -p models

          python3 << 'EOF'
          import pandas as pd
          import numpy as np
          from sklearn.model_selection import train_test_split
          from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
          from sklearn.linear_model import LogisticRegression
          from sklearn.tree import DecisionTreeClassifier
          from sklearn.neighbors import KNeighborsClassifier
          from sklearn.naive_bayes import GaussianNB
          from sklearn.svm import SVC
          from sklearn.neural_network import MLPClassifier
          from catboost import CatBoostClassifier
          from lightgbm import LGBMClassifier
          from xgboost import XGBClassifier
          import joblib
          import json
          import warnings
          warnings.filterwarnings('ignore')

          models = {
              'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
              'gradient_boost': GradientBoostingClassifier(n_estimators=100, random_state=42),
              'adaboost': AdaBoostClassifier(n_estimators=100, random_state=42),
              'logistic': LogisticRegression(max_iter=1000, random_state=42),
              'decision_tree': DecisionTreeClassifier(random_state=42),
              'knn': KNeighborsClassifier(n_neighbors=5),
              'naive_bayes': GaussianNB(),
              'svm': SVC(probability=True, random_state=42),
              'mlp': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),
              'catboost': CatBoostClassifier(iterations=100, random_state=42, verbose=0),
              'lightgbm': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),
              'xgboost': XGBClassifier(n_estimators=100, random_state=42, verbosity=0),
              'extra_trees': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, criterion='entropy'),
              'bagging': AdaBoostClassifier(n_estimators=50, random_state=42),
              'voting_base': RandomForestClassifier(n_estimators=50, random_state=42),
          }

          results = {'url': {}, 'dns': {}, 'whois': {}}

          for feature_type in ['url', 'dns', 'whois']:
              print(f"\n{'='*60}")
              print(f"Training {feature_type.upper()} models...")
              print(f"{'='*60}")

              df = pd.read_csv(f'data/processed/{feature_type}_features_modelready_imputed.csv')
              X = df.drop('label_encoded', axis=1)
              y = df['label_encoded']

              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

              for name, model in models.items():
                  try:
                      model.fit(X_train, y_train)
                      accuracy = model.score(X_test, y_test)
                      results[feature_type][name] = accuracy

                      joblib.dump(model, f'models/{feature_type}_{name}.pkl')
                      print(f"  âœ“ {name}: {accuracy:.4f}")
                  except Exception as e:
                      print(f"  âœ— {name}: {e}")

          # Save results
          with open('models/training_results.json', 'w') as f:
              json.dump(results, f, indent=2)

          # Find best models
          print(f"\n{'='*60}")
          print("BEST MODELS:")
          print(f"{'='*60}")
          for feature_type in ['url', 'dns', 'whois']:
              if results[feature_type]:
                  best = max(results[feature_type].items(), key=lambda x: x[1])
                  print(f"  {feature_type.upper()}: {best[0]} ({best[1]:.4f})")

          print("\nâœ… All 45 models trained")
          EOF

          echo "trained=true" >> $GITHUB_OUTPUT

      # --------------------------------------------------------------------
      # STEP 5: Evaluate ensemble
      # --------------------------------------------------------------------
      - name: Evaluate ensemble
        if: steps.check.outputs.should_run == 'true'
        id: ensemble
        run: |
          echo "ğŸ”— Evaluating ensemble combinations..."

          python3 scripts/ensemble_comparison.py --test-size 500 --iterations 20 || {
            echo "âš ï¸ Ensemble comparison failed, using fallback"
          }

          # Get best result
          LATEST=$(ls -t analysis/ensemble_comparison/comparison_*.json 2>/dev/null | head -1)
          if [ -n "$LATEST" ]; then
            cp "$LATEST" models/best_ensemble.json

            python3 << EOF
          import json
          with open('models/best_ensemble.json') as f:
              results = json.load(f)
          if isinstance(results, list) and len(results) > 0:
              best = results[0]
              print(f"ğŸ† Best: {best.get('name', 'Unknown')}")
              print(f"   Accuracy: {best.get('accuracy', 0):.4f}")
              print(f"   F1: {best.get('f1_score', 0):.4f}")
          EOF
          fi

      # --------------------------------------------------------------------
      # STEP 6: Record validation results
      # --------------------------------------------------------------------
      - name: Record validation results
        if: steps.check.outputs.should_run == 'true'
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ğŸ“Š VALIDATION RUN ${{ matrix.run_num }} COMPLETE"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "Dataset Growth:"
          echo "  Before: ${{ steps.master.outputs.existing_urls }} URLs"
          echo "  Added:  ${{ steps.collect.outputs.total_urls }} NEW URLs"
          echo "  After:  ${{ steps.updated_master.outputs.new_total }} total URLs"
          echo ""
          echo "URLs Collected (Run ${{ matrix.run_num }}):"
          echo "  Phishing:   ${{ steps.collect.outputs.phishing_count }}"
          echo "  Legitimate: ${{ steps.collect.outputs.legit_count }}"
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

      - name: Upload run artifacts
        if: steps.check.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: validation-run-${{ matrix.run_num }}-${{ needs.init.outputs.start_time }}
          path: |
            models/training_results.json
            models/best_ensemble.json
            analysis/ensemble_comparison/
          retention-days: 30

  # ========================================================================
  # Final summary
  # ========================================================================
  summary:
    name: "Final Summary"
    runs-on: ubuntu-latest
    needs: [init, validation-run]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download final master
        run: |
          mkdir -p data/processed
          aws s3 cp s3://${{ env.S3_BUCKET }}/master/phishing_features_master.csv data/processed/phishing_features_complete.csv

      - name: Print final summary
        run: |
          TOTAL_RUNS="${{ needs.init.outputs.total_runs }}"
          URLS_PER_RUN="${{ github.event.inputs.urls_per_run || '100' }}"
          FINAL_COUNT=$(tail -n +2 data/processed/phishing_features_complete.csv | wc -l | tr -d ' ')

          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘           10-RUN VALIDATION COMPLETE                         â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""
          echo "Configuration:"
          echo "  Total runs:     $TOTAL_RUNS"
          echo "  URLs per run:   $URLS_PER_RUN"
          echo "  Expected total: $((TOTAL_RUNS * URLS_PER_RUN)) new URLs"
          echo ""
          echo "Results:"
          echo "  Final dataset:  $FINAL_COUNT total URLs"
          echo ""
          echo "The models have progressively learned from:"
          echo "  â†’ $TOTAL_RUNS batches of $URLS_PER_RUN NEW unique URLs"
          echo "  â†’ Each batch excluded duplicates from previous runs"
          echo "  â†’ 45 models trained on growing dataset each iteration"
          echo ""
          echo "Download artifacts to see per-run accuracy progression!"
          echo ""
