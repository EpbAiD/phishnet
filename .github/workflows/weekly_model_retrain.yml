name: Weekly Model Retraining

on:
  # Run weekly on Sundays at 9 AM EST (14:00 UTC)
  schedule:
    - cron: '0 14 * * 0'
  workflow_dispatch:  # Allow manual trigger
    inputs:
      days_to_merge:
        description: 'Number of days of data to merge (default: 7)'
        required: false
        default: '7'

env:
  GCP_PROJECT_ID: coms-452404
  GCP_ZONE: us-central1-c
  VM_NAME: dns-whois-fetch-25
  GCP_ACCOUNT: eb3658@columbia.edu

jobs:
  merge-weekly-data:
    name: Merge Weekly Data from VM
    runs-on: ubuntu-latest
    outputs:
      needs_retraining: ${{ steps.prepare.outputs.needs_retraining }}
      data_merged: ${{ steps.prepare.outputs.success }}
      total_rows: ${{ steps.prepare.outputs.total_rows }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup GCloud CLI
        uses: google-github-actions/setup-gcloud@v2

      - name: Start VM if needed
        run: |
          VM_STATUS=$(gcloud compute instances describe ${{ env.VM_NAME }} \
            --zone=${{ env.GCP_ZONE }} \
            --format="get(status)")

          if [ "$VM_STATUS" = "TERMINATED" ]; then
            echo "Starting VM..."
            gcloud compute instances start ${{ env.VM_NAME }} --zone=${{ env.GCP_ZONE }}

            echo "Waiting for VM to be ready..."
            sleep 30
          else
            echo "VM is already running (status: $VM_STATUS)"
          fi

      - name: Download weekly data from GCS
        id: download
        run: |
          DAYS_TO_MERGE="${{ github.event.inputs.days_to_merge || '7' }}"

          echo "Downloading last $DAYS_TO_MERGE days of incremental data from GCS..."
          echo "Using merge_gcs_data.py script"

          # The merge script downloads from GCS and also picks up local URL batches
          python3 scripts/merge_gcs_data.py $DAYS_TO_MERGE

          echo "Downloads and merge complete!"

      - name: Prepare model-ready datasets with intelligent detection
        id: prepare
        run: |
          # This script checks for data growth and prepares model-ready data
          # Returns exit 0 if retraining needed, exit 1 if not
          if python3 scripts/prepare_modelready.py; then
            echo "Data growth detected - model-ready datasets prepared"
            echo "needs_retraining=true" >> $GITHUB_OUTPUT

            # Validate data quality
            echo "Validating data quality..."
            python3 scripts/validate_data_quality.py data/processed/url_features_modelready.csv url
            python3 scripts/validate_data_quality.py data/processed/dns_features_modelready.csv dns
            python3 scripts/validate_data_quality.py data/processed/whois_features_modelready.csv whois

            # Get row counts
            DNS_ROWS=$(tail -n +2 data/processed/dns_features_modelready.csv | wc -l | tr -d ' ')
            echo "success=true" >> $GITHUB_OUTPUT
            echo "total_rows=$DNS_ROWS" >> $GITHUB_OUTPUT
          else
            echo "No data growth - skipping retraining"
            echo "needs_retraining=false" >> $GITHUB_OUTPUT
            echo "success=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload model-ready data as artifact
        if: steps.prepare.outputs.needs_retraining == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: weekly-modelready-data-${{ github.run_number }}
          path: data/processed/*_modelready*.csv
          retention-days: 30

      - name: Stop VM
        if: always()
        run: |
          VM_STATUS=$(gcloud compute instances describe ${{ env.VM_NAME }} \
            --zone=${{ env.GCP_ZONE }} \
            --format="get(status)")

          if [ "$VM_STATUS" = "RUNNING" ]; then
            echo "Stopping VM..."
            gcloud compute instances stop ${{ env.VM_NAME }} --zone=${{ env.GCP_ZONE }}
            echo "âœ… VM stopped"
          fi

  retrain-models:
    name: Retrain All Models with Weekly Data
    runs-on: ubuntu-latest
    needs: merge-weekly-data
    if: needs.merge-weekly-data.outputs.needs_retraining == 'true'
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download model-ready data
        uses: actions/download-artifact@v4
        with:
          name: weekly-modelready-data-${{ github.run_number }}
          path: data/processed/

      - name: Train URL model
        run: python3 scripts/train_url_model.py

      - name: Train DNS model
        run: python3 scripts/train_dns_model.py

      - name: Train WHOIS model
        run: python3 scripts/train_whois_model.py

      - name: Upload trained models as artifact
        uses: actions/upload-artifact@v4
        with:
          name: weekly-trained-models-${{ github.run_number }}
          path: |
            models/*.pkl
            models/*.joblib
            models/production_metadata.json
          retention-days: 90

      - name: Commit updated models and data to repository
        run: |
          git config user.name "EpbAiD"
          git config user.email "eeshanpbhanap@gmail.com"

          # Add trained models
          git add models/

          # Add model-ready datasets (downloaded from artifact)
          git add data/processed/*_modelready*.csv || true

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          # Commit with informative message
          TOTAL_ROWS="${{ needs.merge-weekly-data.outputs.total_rows }}"
          git commit -m "Weekly model retraining with ${TOTAL_ROWS} total rows

          - Retrained all models (URL, DNS, WHOIS)
          - Models saved to models/ directory
          - Automated via GitHub Actions

          ðŸ¤– Generated with Claude Code
          Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

          git push

  trigger-performance-monitor:
    name: Trigger Performance Monitoring
    runs-on: ubuntu-latest
    needs: retrain-models
    if: needs.retrain-models.result == 'success'

    steps:
      - name: Trigger model performance monitor
        uses: actions/github-script@v6
        with:
          script: |
            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'model_performance_monitor.yml',
              ref: 'main'
            });
            console.log('âœ… Triggered model performance monitor workflow');
