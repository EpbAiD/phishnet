name: Daily Pipeline (End-to-End)

on:
  # schedule:
  #   # DISABLED - Testing phase, will enable after rigorous testing
  #   - cron: '0 14 * * *'
  workflow_dispatch:
    inputs:
      num_urls:
        description: 'Number of URLs to collect'
        required: false
        default: '1000'

env:
  GCP_PROJECT_ID: coms-452404
  GCP_ZONE: us-central1-c
  VM_NAME: dns-whois-fetch-25

jobs:
  run-pipeline:
    name: Complete Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 600  # 10 hours max

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup GCloud CLI
        uses: google-github-actions/setup-gcloud@v2

      # ========================================================================
      # STEP 1: Collect URLs
      # ========================================================================
      - name: Collect URLs
        run: |
          NUM_URLS="${{ github.event.inputs.num_urls || '1000' }}"
          BATCH_DATE=$(date +%Y%m%d)
          BATCH_FILE="data/url_queue/batch_${BATCH_DATE}.csv"

          mkdir -p data/url_queue logs

          echo "========================================" | tee logs/step1_collect.log
          echo "STEP 1: Collecting $NUM_URLS URLs" | tee -a logs/step1_collect.log
          echo "========================================" | tee -a logs/step1_collect.log

          python3 scripts/fetch_urls.py $BATCH_FILE $NUM_URLS 2>&1 | tee -a logs/step1_collect.log

          echo "batch_file=$BATCH_FILE" >> $GITHUB_ENV
          echo "batch_name=batch_${BATCH_DATE}.csv" >> $GITHUB_ENV
          echo "batch_date=$BATCH_DATE" >> $GITHUB_ENV

          COLLECTED=$(wc -l < $BATCH_FILE)
          echo "✅ Collected: $COLLECTED URLs" | tee -a logs/step1_collect.log
          echo "File: $BATCH_FILE" | tee -a logs/step1_collect.log
          head -5 $BATCH_FILE | tee -a logs/step1_collect.log

      # ========================================================================
      # STEP 2: Extract URL Features (local)
      # ========================================================================
      - name: Extract URL Features
        run: |
          mkdir -p data/processed

          echo "========================================" | tee logs/step2_url_features.log
          echo "STEP 2: Extracting URL Features" | tee -a logs/step2_url_features.log
          echo "========================================" | tee -a logs/step2_url_features.log

          python3 scripts/extract_url_features.py \
            ${{ env.batch_file }} \
            data/processed/phishing_features_complete_url.csv 2>&1 | tee -a logs/step2_url_features.log

          ROWS=$(wc -l < data/processed/phishing_features_complete_url.csv)
          echo "✅ URL features: $ROWS rows" | tee -a logs/step2_url_features.log
          head -3 data/processed/phishing_features_complete_url.csv | tee -a logs/step2_url_features.log

      # ========================================================================
      # STEP 3: Start VM & Extract DNS/WHOIS Features
      # ========================================================================
      - name: Start VM
        run: |
          VM_STATUS=$(gcloud compute instances describe ${{ env.VM_NAME }} \
            --zone=${{ env.GCP_ZONE }} \
            --format="get(status)")

          if [ "$VM_STATUS" = "TERMINATED" ]; then
            echo "Starting VM..."
            gcloud compute instances start ${{ env.VM_NAME }} --zone=${{ env.GCP_ZONE }}
            echo "Waiting for VM to be ready..."
            sleep 90
          else
            echo "VM already running"
            sleep 10
          fi

      - name: Extract DNS/WHOIS features on VM
        run: |
          BATCH_DATE="${{ env.batch_date }}"
          BATCH_FILE="${{ env.batch_file }}"
          BATCH_NAME="${{ env.batch_name }}"

          echo "========================================" | tee logs/step3_vm_features.log
          echo "STEP 3: Extracting DNS/WHOIS Features on VM" | tee -a logs/step3_vm_features.log
          echo "========================================" | tee -a logs/step3_vm_features.log

          echo "Uploading to GCS..." | tee -a logs/step3_vm_features.log
          gcloud storage cp $BATCH_FILE gs://phishnet-pipeline-data/queue/$BATCH_NAME 2>&1 | tee -a logs/step3_vm_features.log
          gcloud storage cp scripts/extract_vm_features.py gs://phishnet-pipeline-data/scripts/ 2>&1 | tee -a logs/step3_vm_features.log

          echo "Processing on VM..." | tee -a logs/step3_vm_features.log
          gcloud compute ssh ${{ env.VM_NAME }} \
            --zone=${{ env.GCP_ZONE }} \
            --command="sudo -u eeshanbhanap bash -c 'cd /home/eeshanbhanap/phishnet && gcloud storage cp gs://phishnet-pipeline-data/scripts/extract_vm_features.py scripts/ && gcloud storage cp gs://phishnet-pipeline-data/queue/$BATCH_NAME vm_data/url_queue/ && python3 scripts/extract_vm_features.py vm_data/url_queue/$BATCH_NAME $BATCH_DATE && gcloud storage cp vm_data/incremental/dns_${BATCH_DATE}.csv gs://phishnet-pipeline-data/incremental/ && gcloud storage cp vm_data/incremental/whois_${BATCH_DATE}.csv gs://phishnet-pipeline-data/incremental/'" 2>&1 | tee -a logs/step3_vm_features.log

          echo "Downloading results..." | tee -a logs/step3_vm_features.log
          mkdir -p vm_data/incremental
          gcloud storage cp gs://phishnet-pipeline-data/incremental/dns_${BATCH_DATE}.csv vm_data/incremental/ 2>&1 | tee -a logs/step3_vm_features.log
          gcloud storage cp gs://phishnet-pipeline-data/incremental/whois_${BATCH_DATE}.csv vm_data/incremental/ 2>&1 | tee -a logs/step3_vm_features.log

          DNS_ROWS=$(wc -l < vm_data/incremental/dns_${BATCH_DATE}.csv)
          WHOIS_ROWS=$(wc -l < vm_data/incremental/whois_${BATCH_DATE}.csv)
          echo "✅ DNS features: $DNS_ROWS rows" | tee -a logs/step3_vm_features.log
          echo "✅ WHOIS features: $WHOIS_ROWS rows" | tee -a logs/step3_vm_features.log

      - name: Stop VM
        if: always()
        run: |
          gcloud compute instances stop ${{ env.VM_NAME }} --zone=${{ env.GCP_ZONE }} || true

      # ========================================================================
      # STEP 4: Merge All Features
      # ========================================================================
      - name: Merge features
        run: |
          BATCH_DATE="${{ env.batch_date }}"

          echo "Merging URL + DNS + WHOIS features..."
          python3 scripts/merge_features.py \
            data/processed/phishing_features_complete_url.csv \
            vm_data/incremental/dns_${BATCH_DATE}.csv \
            vm_data/incremental/whois_${BATCH_DATE}.csv \
            data/processed/phishing_features_complete.csv

          echo "✅ All features merged"
          wc -l data/processed/phishing_features_complete.csv

      # ========================================================================
      # STEP 5: Prepare model-ready data
      # ========================================================================
      - name: Prepare model-ready datasets
        run: |
          echo "Preparing model-ready datasets..."

          # For simplicity, just copy the complete file to modelready files
          # Each training script will select its own features
          cp data/processed/phishing_features_complete.csv data/processed/url_features_modelready.csv
          cp data/processed/phishing_features_complete.csv data/processed/dns_features_modelready.csv
          cp data/processed/phishing_features_complete.csv data/processed/whois_features_modelready.csv

          echo "✅ Model-ready data prepared"
          wc -l data/processed/url_features_modelready.csv
          wc -l data/processed/dns_features_modelready.csv
          wc -l data/processed/whois_features_modelready.csv

      # ========================================================================
      # STEP 6: Train models
      # ========================================================================
      - name: Train URL model
        run: |
          echo "Training URL model..."
          python3 scripts/train_url_model.py data/processed/url_features_modelready.csv
          echo "✅ URL model trained"

      - name: Train DNS model
        run: |
          echo "Training DNS model..."
          python3 scripts/train_dns_model.py data/processed/dns_features_modelready.csv
          echo "✅ DNS model trained"

      - name: Train WHOIS model
        run: |
          echo "Training WHOIS model..."
          python3 scripts/train_whois_model.py data/processed/whois_features_modelready.csv
          echo "✅ WHOIS model trained"

      # ========================================================================
      # STEP 7: Commit models to GitHub
      # ========================================================================
      - name: Commit trained models
        run: |
          git config user.name "EpbAiD"
          git config user.email "eeshanpbhanap@gmail.com"

          git add models/

          if git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          NUM_URLS="${{ github.event.inputs.num_urls || '1000' }}"
          git commit -m "Daily pipeline: train models with $NUM_URLS URLs

          - Collected $NUM_URLS URLs from sources
          - Extracted URL + DNS + WHOIS features
          - Trained all models (URL, DNS, WHOIS)"

          git push

          echo "✅ Models committed to GitHub"

      # ========================================================================
      # STEP 8: Upload test results
      # ========================================================================
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-test-results-${{ github.run_number }}
          path: |
            logs/
            data/url_queue/batch_*.csv
            data/processed/phishing_features_complete*.csv
            vm_data/incremental/
            models/production_metadata.json
          retention-days: 30

      # ========================================================================
      # STEP 9: Create summary
      # ========================================================================
      - name: Create test summary
        if: always()
        run: |
          echo "========================================" | tee logs/summary.log
          echo "PIPELINE TEST SUMMARY" | tee -a logs/summary.log
          echo "========================================" | tee -a logs/summary.log
          echo "Run: ${{ github.run_number }}" | tee -a logs/summary.log
          echo "URLs requested: ${{ github.event.inputs.num_urls }}" | tee -a logs/summary.log
          echo "" | tee -a logs/summary.log

          if [ -f "logs/step1_collect.log" ]; then
            echo "Step 1: URL Collection" | tee -a logs/summary.log
            grep "Collected:" logs/step1_collect.log | tee -a logs/summary.log
          fi

          if [ -f "logs/step2_url_features.log" ]; then
            echo "Step 2: URL Features" | tee -a logs/summary.log
            grep "URL features:" logs/step2_url_features.log | tee -a logs/summary.log
          fi

          if [ -f "logs/step3_vm_features.log" ]; then
            echo "Step 3: VM Features" | tee -a logs/summary.log
            grep "features:" logs/step3_vm_features.log | tee -a logs/summary.log
          fi

          echo "" | tee -a logs/summary.log
          cat logs/summary.log
