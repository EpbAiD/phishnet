name: Daily Pipeline (End-to-End)

on:
  schedule:
    # Run daily at 9 AM EST (14:00 UTC)
    - cron: '0 14 * * *'
  workflow_dispatch:
    inputs:
      num_urls:
        description: 'Number of URLs (20 for test, 1000 for production)'
        required: false
        default: '20'

env:
  GCP_PROJECT_ID: coms-452404
  GCP_ZONE: us-central1-c
  VM_NAME: dns-whois-fetch-25

jobs:
  run-pipeline:
    name: Complete Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 600  # 10 hours max

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup GCloud CLI
        uses: google-github-actions/setup-gcloud@v2

      # ========================================================================
      # STEP 1: Collect URLs
      # ========================================================================
      - name: Collect URLs
        run: |
          NUM_URLS="${{ github.event.inputs.num_urls || '20' }}"
          BATCH_DATE=$(date +%Y%m%d)
          BATCH_FILE="data/url_queue/batch_${BATCH_DATE}.csv"

          mkdir -p data/url_queue

          echo "Collecting $NUM_URLS URLs..."
          python3 scripts/fetch_urls.py $BATCH_FILE $NUM_URLS

          echo "batch_file=$BATCH_FILE" >> $GITHUB_ENV
          echo "batch_name=batch_${BATCH_DATE}.csv" >> $GITHUB_ENV
          echo "batch_date=$BATCH_DATE" >> $GITHUB_ENV

          echo "✅ Collected URLs: $BATCH_FILE"
          wc -l $BATCH_FILE

      # ========================================================================
      # STEP 2: Extract URL Features (local)
      # ========================================================================
      - name: Extract URL Features
        run: |
          echo "Extracting URL features from ${{ env.batch_file }}..."
          python3 scripts/extract_url_features.py ${{ env.batch_file }}

          # This creates data/processed/phishing_features_complete_url.csv
          echo "✅ URL features extracted"
          wc -l data/processed/phishing_features_complete_url.csv

      # ========================================================================
      # STEP 3: Start VM & Extract DNS/WHOIS Features
      # ========================================================================
      - name: Start VM
        run: |
          VM_STATUS=$(gcloud compute instances describe ${{ env.VM_NAME }} \
            --zone=${{ env.GCP_ZONE }} \
            --format="get(status)")

          if [ "$VM_STATUS" = "TERMINATED" ]; then
            echo "Starting VM..."
            gcloud compute instances start ${{ env.VM_NAME }} --zone=${{ env.GCP_ZONE }}
            sleep 60
          else
            echo "VM already running"
          fi

      - name: Upload batch to VM and extract DNS/WHOIS features
        run: |
          BATCH_DATE="${{ env.batch_date }}"
          BATCH_FILE="${{ env.batch_file }}"
          BATCH_NAME="${{ env.batch_name }}"

          echo "Uploading $BATCH_FILE to VM..."
          gcloud compute scp $BATCH_FILE \
            ${{ env.VM_NAME }}:/home/eeshanbhanap/phishnet/vm_data/url_queue/$BATCH_NAME \
            --zone=${{ env.GCP_ZONE }}

          echo "Running feature extraction on VM..."
          gcloud compute ssh ${{ env.VM_NAME }} \
            --zone=${{ env.GCP_ZONE }} \
            --command="cd /home/eeshanbhanap/phishnet && python3 scripts/extract_vm_features.py vm_data/url_queue/$BATCH_NAME $BATCH_DATE"

          # This creates:
          # - vm_data/incremental/dns_YYYYMMDD.csv
          # - vm_data/incremental/whois_YYYYMMDD.csv

          echo "Downloading results from VM..."
          mkdir -p vm_data/incremental

          gcloud compute scp \
            ${{ env.VM_NAME }}:/home/eeshanbhanap/phishnet/vm_data/incremental/dns_${BATCH_DATE}.csv \
            vm_data/incremental/ \
            --zone=${{ env.GCP_ZONE }}

          gcloud compute scp \
            ${{ env.VM_NAME }}:/home/eeshanbhanap/phishnet/vm_data/incremental/whois_${BATCH_DATE}.csv \
            vm_data/incremental/ \
            --zone=${{ env.GCP_ZONE }}

          echo "✅ DNS/WHOIS features extracted"
          wc -l vm_data/incremental/dns_${BATCH_DATE}.csv
          wc -l vm_data/incremental/whois_${BATCH_DATE}.csv

      - name: Stop VM
        if: always()
        run: |
          gcloud compute instances stop ${{ env.VM_NAME }} --zone=${{ env.GCP_ZONE }} || true

      # ========================================================================
      # STEP 4: Merge All Features
      # ========================================================================
      - name: Merge features
        run: |
          BATCH_DATE="${{ env.batch_date }}"

          echo "Merging URL + DNS + WHOIS features..."
          python3 scripts/merge_features.py \
            data/processed/phishing_features_complete_url.csv \
            vm_data/incremental/dns_${BATCH_DATE}.csv \
            vm_data/incremental/whois_${BATCH_DATE}.csv \
            data/processed/phishing_features_complete.csv

          echo "✅ All features merged"
          wc -l data/processed/phishing_features_complete.csv

      # ========================================================================
      # STEP 5: Prepare model-ready data
      # ========================================================================
      - name: Prepare model-ready datasets
        run: |
          echo "Preparing model-ready datasets..."
          python3 scripts/prepare_modelready.py data/processed/phishing_features_complete.csv

          # This creates:
          # - data/processed/url_features_modelready.csv
          # - data/processed/dns_features_modelready.csv
          # - data/processed/whois_features_modelready.csv

          echo "✅ Model-ready data prepared"
          wc -l data/processed/url_features_modelready.csv
          wc -l data/processed/dns_features_modelready.csv
          wc -l data/processed/whois_features_modelready.csv

      # ========================================================================
      # STEP 6: Train models
      # ========================================================================
      - name: Train URL model
        run: |
          echo "Training URL model..."
          python3 scripts/train_url_model.py data/processed/url_features_modelready.csv
          echo "✅ URL model trained"

      - name: Train DNS model
        run: |
          echo "Training DNS model..."
          python3 scripts/train_dns_model.py data/processed/dns_features_modelready.csv
          echo "✅ DNS model trained"

      - name: Train WHOIS model
        run: |
          echo "Training WHOIS model..."
          python3 scripts/train_whois_model.py data/processed/whois_features_modelready.csv
          echo "✅ WHOIS model trained"

      # ========================================================================
      # STEP 7: Commit models to GitHub
      # ========================================================================
      - name: Commit trained models
        run: |
          git config user.name "EpbAiD"
          git config user.email "eeshanpbhanap@gmail.com"

          git add models/

          if git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          NUM_URLS="${{ github.event.inputs.num_urls || '20' }}"
          git commit -m "Daily pipeline: train models with $NUM_URLS URLs

          - Collected $NUM_URLS URLs from sources
          - Extracted URL + DNS + WHOIS features
          - Trained all models (URL, DNS, WHOIS)

          Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

          git push

          echo "✅ Models committed to GitHub"

      # ========================================================================
      # STEP 8: Deploy to Cloud Run (if needed)
      # ========================================================================
      - name: Deploy API
        run: |
          echo "Deploying API to Cloud Run..."
          gcloud run deploy phishnet-api \
            --source . \
            --region us-central1 \
            --platform managed \
            --allow-unauthenticated \
            --set-env-vars="GCP_PROJECT_ID=${{ env.GCP_PROJECT_ID }}" || echo "Deployment skipped"

          echo "✅ Pipeline complete!"
